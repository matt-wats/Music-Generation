# Music-Generation
Using Autoregressive Transformers to generate classical piano music


# DEMO
Example output of song generated by program:

https://github.com/matt-wats/Music-Generation/assets/112960646/6e4b8c3f-4a6c-4212-9c09-127641b48d0a



# Motivation

I wanted to get a better understand of how language models work, but wanted to do something interesting.
I like classical music, but haven't seen much in the way of using machine learning to create them, so I did!



# How it Works

## Getting Data
I scraped all of the music from http://www.piano-midi.de/


## Basic Idea
The model is given the previous notes as input, and outputs probabilites for what the next note in the sequence would be.
The sequence of notes it is given it just notes, not chords. Each song is split up into a sequence of notes, where chords are split into notes that are played "at the same time".
I did this because there were a great number of chords that are rare, which if included, would need a lot of memeory for embeddings and create wonky predictions.
In my mind, this is similar to a character-level model for text generation.

## Interesting Ideas
Notes have properties beyond just what key is pressed on the piano. They also have a duration, volume, and offset (how long after the previous key is pressed this one is pressed; i.e. an offset of 0 would mean it's pressed at the some time: a chord)
Treating each note with a unique set of properties is possible, but I wanted notes with similar properties to BE similar inputs. So, I used the note properties as input.
To do this, I had each property correspond to a different embeddings dictionary with different dimensions (255 for pitch, 64 for duration, 64 for offset, and 1 for volume), and are combined to create a 384-dimension vector as input.
In this model, the outputs are the unique notes, not their properties (which I did try, and it works, but the idea is dubious at best).


# Results

Here is the plots of the Training Set Loss every epoch, Validation Set Loss every 10th epoch, and Learning rate every epoch:
![Losses and Learning Rates](https://github.com/matt-wats/Music-Generation/blob/main/Data/Figure_1.png "Losses and Learning Rates")

The model can get really good at predicting the training set! Unfortunately, the model is bad at generalizing it's predictions. 
We can use this method to create realistic seeming classical music, but it shouldn't be used to create "new" music.


# Bonus Stuff

I also tested the models on just the Chopin dataset using the properties as inputs, as well as the unique notes as inputs.
![Losses and Learning Rates](https://github.com/matt-wats/Music-Generation/blob/main/Data/Comparison_Properties.png "Losses and Learning Rates")

![Losses and Learning Rates](https://github.com/matt-wats/Music-Generation/blob/main/Data/Comparison_Unique.png "Losses and Learning Rates")

Using the properties as inputs give us a slightly better generalization on the validation set, but not much :(.


# Future Improvements

- Collecting more data to improve the model's ability to predict different styles of composing
- Tune hyperparameters: A lower learning rate corresponds to better performance on the validation set, etc.
- Use a GAN style training, if our goal is to create new music (not "quote" other songs)
- Use a better note generation algorithm, like Beam Search, to improve outputs
